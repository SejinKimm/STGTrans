{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.1+cu102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sjkim/anaconda3/envs/STGTrans/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Parameter\n",
    "from torch_geometric.nn import ChebConv\n",
    "from torch_geometric.nn.inits import glorot, zeros\n",
    "\n",
    "\n",
    "class GCLSTM(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        in_channels (int): Number of input features.\n",
    "        out_channels (int): Number of output features.\n",
    "        K (int): Chebyshev filter size :math:`K`.\n",
    "        normalization (str, optional): The normalization scheme for the graph\n",
    "            Laplacian (default: :obj:`\"sym\"`):\n",
    "\n",
    "            1. :obj:`None`: No normalization\n",
    "            :math:`\\mathbf{L} = \\mathbf{D} - \\mathbf{A}`\n",
    "\n",
    "            2. :obj:`\"sym\"`: Symmetric normalization\n",
    "            :math:`\\mathbf{L} = \\mathbf{I} - \\mathbf{D}^{-1/2} \\mathbf{A}\n",
    "            \\mathbf{D}^{-1/2}`\n",
    "\n",
    "            3. :obj:`\"rw\"`: Random-walk normalization\n",
    "            :math:`\\mathbf{L} = \\mathbf{I} - \\mathbf{D}^{-1} \\mathbf{A}`\n",
    "\n",
    "            You need to pass :obj:`lambda_max` to the :meth:`forward` method of\n",
    "            this operator in case the normalization is non-symmetric.\n",
    "            :obj:`\\lambda_max` should be a :class:`torch.Tensor` of size\n",
    "            :obj:`[num_graphs]` in a mini-batch scenario and a\n",
    "            scalar/zero-dimensional tensor when operating on single graphs.\n",
    "            You can pre-compute :obj:`lambda_max` via the\n",
    "            :class:`torch_geometric.transforms.LaplacianLambdaMax` transform.\n",
    "        bias (bool, optional): If set to :obj:`False`, the layer will not learn\n",
    "            an additive bias. (default: :obj:`True`)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        K: int,\n",
    "        normalization: str = \"sym\",\n",
    "        bias: bool = True,\n",
    "    ):\n",
    "        super(GCLSTM, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.K = K\n",
    "        self.normalization = normalization\n",
    "        self.bias = bias\n",
    "        self._create_parameters_and_layers()\n",
    "        self._set_parameters()\n",
    "\n",
    "    def _create_input_gate_parameters_and_layers(self):\n",
    "\n",
    "        self.conv_i = ChebConv(\n",
    "            in_channels=self.out_channels,\n",
    "            out_channels=self.out_channels,\n",
    "            K=self.K,\n",
    "            normalization=self.normalization,\n",
    "            bias=self.bias,\n",
    "        )\n",
    "\n",
    "        self.W_i = Parameter(torch.Tensor(self.in_channels, self.out_channels))\n",
    "        self.b_i = Parameter(torch.Tensor(1, self.out_channels))\n",
    "\n",
    "    def _create_forget_gate_parameters_and_layers(self):\n",
    "\n",
    "        self.conv_f = ChebConv(\n",
    "            in_channels=self.out_channels,\n",
    "            out_channels=self.out_channels,\n",
    "            K=self.K,\n",
    "            normalization=self.normalization,\n",
    "            bias=self.bias,\n",
    "        )\n",
    "\n",
    "        self.W_f = Parameter(torch.Tensor(self.in_channels, self.out_channels))\n",
    "        self.b_f = Parameter(torch.Tensor(1, self.out_channels))\n",
    "\n",
    "    def _create_cell_state_parameters_and_layers(self):\n",
    "\n",
    "        self.conv_c = ChebConv(\n",
    "            in_channels=self.out_channels,\n",
    "            out_channels=self.out_channels,\n",
    "            K=self.K,\n",
    "            normalization=self.normalization,\n",
    "            bias=self.bias,\n",
    "        )\n",
    "\n",
    "        self.W_c = Parameter(torch.Tensor(self.in_channels, self.out_channels))\n",
    "        self.b_c = Parameter(torch.Tensor(1, self.out_channels))\n",
    "\n",
    "    def _create_output_gate_parameters_and_layers(self):\n",
    "\n",
    "        self.conv_o = ChebConv(\n",
    "            in_channels=self.out_channels,\n",
    "            out_channels=self.out_channels,\n",
    "            K=self.K,\n",
    "            normalization=self.normalization,\n",
    "            bias=self.bias,\n",
    "        )\n",
    "\n",
    "        self.W_o = Parameter(torch.Tensor(self.in_channels, self.out_channels))\n",
    "        self.b_o = Parameter(torch.Tensor(1, self.out_channels))\n",
    "\n",
    "    def _create_parameters_and_layers(self):\n",
    "        self._create_input_gate_parameters_and_layers()\n",
    "        self._create_forget_gate_parameters_and_layers()\n",
    "        self._create_cell_state_parameters_and_layers()\n",
    "        self._create_output_gate_parameters_and_layers()\n",
    "\n",
    "    def _set_parameters(self):\n",
    "        glorot(self.W_i)\n",
    "        glorot(self.W_f)\n",
    "        glorot(self.W_c)\n",
    "        glorot(self.W_o)\n",
    "        zeros(self.b_i)\n",
    "        zeros(self.b_f)\n",
    "        zeros(self.b_c)\n",
    "        zeros(self.b_o)\n",
    "\n",
    "    def _set_hidden_state(self, X, H):\n",
    "        if H is None:\n",
    "            H = torch.zeros(X.shape[0], self.out_channels).to(X.device)\n",
    "        return H\n",
    "\n",
    "    def _set_cell_state(self, X, C):\n",
    "        if C is None:\n",
    "            C = torch.zeros(X.shape[0], self.out_channels).to(X.device)\n",
    "        return C\n",
    "\n",
    "    def _calculate_input_gate(self, X, edge_index, edge_weight, H, C, lambda_max):\n",
    "        I = torch.matmul(X, self.W_i)\n",
    "        I = I + self.conv_i(H, edge_index, edge_weight, lambda_max=lambda_max)\n",
    "        I = I + self.b_i\n",
    "        I = torch.sigmoid(I)\n",
    "        return I\n",
    "\n",
    "    def _calculate_forget_gate(self, X, edge_index, edge_weight, H, C, lambda_max):\n",
    "        F = torch.matmul(X, self.W_f)\n",
    "        F = F + self.conv_f(H, edge_index, edge_weight, lambda_max=lambda_max)\n",
    "        F = F + self.b_f\n",
    "        F = torch.sigmoid(F)\n",
    "        return F\n",
    "\n",
    "    def _calculate_cell_state(self, X, edge_index, edge_weight, H, C, I, F, lambda_max):\n",
    "        T = torch.matmul(X, self.W_c)\n",
    "        T = T + self.conv_c(H, edge_index, edge_weight, lambda_max=lambda_max)\n",
    "        T = T + self.b_c\n",
    "        T = torch.tanh(T)\n",
    "        C = F * C + I * T\n",
    "        return C\n",
    "\n",
    "    def _calculate_output_gate(self, X, edge_index, edge_weight, H, C, lambda_max):\n",
    "        O = torch.matmul(X, self.W_o)\n",
    "        O = O + self.conv_o(H, edge_index, edge_weight, lambda_max=lambda_max)\n",
    "        O = O + self.b_o\n",
    "        O = torch.sigmoid(O)\n",
    "        return O\n",
    "\n",
    "    def _calculate_hidden_state(self, O, C):\n",
    "        H = O * torch.tanh(C)\n",
    "        return H\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        X: torch.FloatTensor,\n",
    "        edge_index: torch.LongTensor,\n",
    "        edge_weight: torch.FloatTensor = None,\n",
    "        H: torch.FloatTensor = None,\n",
    "        C: torch.FloatTensor = None,\n",
    "        lambda_max: torch.Tensor = None,\n",
    "    ) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        Making a forward pass. If edge weights are not present the forward pass\n",
    "        defaults to an unweighted graph. If the hidden state and cell state\n",
    "        matrices are not present when the forward pass is called these are\n",
    "        initialized with zeros.\n",
    "\n",
    "        Arg types:\n",
    "            * **X** *(PyTorch Float Tensor)* - Node features.\n",
    "            * **edge_index** *(PyTorch Long Tensor)* - Graph edge indices.\n",
    "            * **edge_weight** *(PyTorch Long Tensor, optional)* - Edge weight vector.\n",
    "            * **H** *(PyTorch Float Tensor, optional)* - Hidden state matrix for all nodes.\n",
    "            * **C** *(PyTorch Float Tensor, optional)* - Cell state matrix for all nodes.\n",
    "            * **lambda_max** *(PyTorch Tensor, optional but mandatory if normalization is not sym)* - Largest eigenvalue of Laplacian.\n",
    "\n",
    "        Return types:\n",
    "            * **H** *(PyTorch Float Tensor)* - Hidden state matrix for all nodes.\n",
    "            * **C** *(PyTorch Float Tensor)* - Cell state matrix for all nodes.\n",
    "        \"\"\"\n",
    "        H = self._set_hidden_state(X, H)\n",
    "        C = self._set_cell_state(X, C)\n",
    "        I = self._calculate_input_gate(X, edge_index, edge_weight, H, C, lambda_max)\n",
    "        F = self._calculate_forget_gate(X, edge_index, edge_weight, H, C, lambda_max)\n",
    "        C = self._calculate_cell_state(X, edge_index, edge_weight, H, C, I, F, lambda_max)\n",
    "        O = self._calculate_output_gate(X, edge_index, edge_weight, H, C, lambda_max)\n",
    "        H = self._calculate_hidden_state(O, C)\n",
    "        return H, C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric_temporal.nn.recurrent import GCLSTM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "STGTrans",
   "language": "python",
   "name": "stgtrans"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
