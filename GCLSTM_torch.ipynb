{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.1+cu102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sjkim/anaconda3/envs/STGTrans/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Parameter\n",
    "from torch_geometric.nn import ChebConv\n",
    "from torch_geometric.nn.inits import glorot, zeros\n",
    "\n",
    "\n",
    "class GCLSTM(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        in_channels (int): Number of input features.\n",
    "        out_channels (int): Number of output features.\n",
    "        K (int): Chebyshev filter size :math:`K`.\n",
    "        normalization (str, optional): The normalization scheme for the graph\n",
    "            Laplacian (default: :obj:`\"sym\"`):\n",
    "\n",
    "            1. :obj:`None`: No normalization\n",
    "            :math:`\\mathbf{L} = \\mathbf{D} - \\mathbf{A}`\n",
    "\n",
    "            2. :obj:`\"sym\"`: Symmetric normalization\n",
    "            :math:`\\mathbf{L} = \\mathbf{I} - \\mathbf{D}^{-1/2} \\mathbf{A}\n",
    "            \\mathbf{D}^{-1/2}`\n",
    "\n",
    "            3. :obj:`\"rw\"`: Random-walk normalization\n",
    "            :math:`\\mathbf{L} = \\mathbf{I} - \\mathbf{D}^{-1} \\mathbf{A}`\n",
    "\n",
    "            You need to pass :obj:`lambda_max` to the :meth:`forward` method of\n",
    "            this operator in case the normalization is non-symmetric.\n",
    "            :obj:`\\lambda_max` should be a :class:`torch.Tensor` of size\n",
    "            :obj:`[num_graphs]` in a mini-batch scenario and a\n",
    "            scalar/zero-dimensional tensor when operating on single graphs.\n",
    "            You can pre-compute :obj:`lambda_max` via the\n",
    "            :class:`torch_geometric.transforms.LaplacianLambdaMax` transform.\n",
    "        bias (bool, optional): If set to :obj:`False`, the layer will not learn\n",
    "            an additive bias. (default: :obj:`True`)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        K: int,\n",
    "        normalization: str = \"sym\",\n",
    "        bias: bool = True,\n",
    "    ):\n",
    "        super(GCLSTM, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.K = K\n",
    "        self.normalization = normalization\n",
    "        self.bias = bias\n",
    "        self._create_parameters_and_layers()\n",
    "        self._set_parameters()\n",
    "\n",
    "    def _create_input_gate_parameters_and_layers(self):\n",
    "\n",
    "        self.conv_i = ChebConv(\n",
    "            in_channels=self.out_channels,\n",
    "            out_channels=self.out_channels,\n",
    "            K=self.K,\n",
    "            normalization=self.normalization,\n",
    "            bias=self.bias,\n",
    "        )\n",
    "\n",
    "        self.W_i = Parameter(torch.Tensor(self.in_channels, self.out_channels))\n",
    "        self.b_i = Parameter(torch.Tensor(1, self.out_channels))\n",
    "\n",
    "    def _create_forget_gate_parameters_and_layers(self):\n",
    "\n",
    "        self.conv_f = ChebConv(\n",
    "            in_channels=self.out_channels,\n",
    "            out_channels=self.out_channels,\n",
    "            K=self.K,\n",
    "            normalization=self.normalization,\n",
    "            bias=self.bias,\n",
    "        )\n",
    "\n",
    "        self.W_f = Parameter(torch.Tensor(self.in_channels, self.out_channels))\n",
    "        self.b_f = Parameter(torch.Tensor(1, self.out_channels))\n",
    "\n",
    "    def _create_cell_state_parameters_and_layers(self):\n",
    "\n",
    "        self.conv_c = ChebConv(\n",
    "            in_channels=self.out_channels,\n",
    "            out_channels=self.out_channels,\n",
    "            K=self.K,\n",
    "            normalization=self.normalization,\n",
    "            bias=self.bias,\n",
    "        )\n",
    "\n",
    "        self.W_c = Parameter(torch.Tensor(self.in_channels, self.out_channels))\n",
    "        self.b_c = Parameter(torch.Tensor(1, self.out_channels))\n",
    "\n",
    "    def _create_output_gate_parameters_and_layers(self):\n",
    "\n",
    "        self.conv_o = ChebConv(\n",
    "            in_channels=self.out_channels,\n",
    "            out_channels=self.out_channels,\n",
    "            K=self.K,\n",
    "            normalization=self.normalization,\n",
    "            bias=self.bias,\n",
    "        )\n",
    "\n",
    "        self.W_o = Parameter(torch.Tensor(self.in_channels, self.out_channels))\n",
    "        self.b_o = Parameter(torch.Tensor(1, self.out_channels))\n",
    "\n",
    "    def _create_parameters_and_layers(self):\n",
    "        self._create_input_gate_parameters_and_layers()\n",
    "        self._create_forget_gate_parameters_and_layers()\n",
    "        self._create_cell_state_parameters_and_layers()\n",
    "        self._create_output_gate_parameters_and_layers()\n",
    "\n",
    "    def _set_parameters(self):\n",
    "        glorot(self.W_i)\n",
    "        glorot(self.W_f)\n",
    "        glorot(self.W_c)\n",
    "        glorot(self.W_o)\n",
    "        zeros(self.b_i)\n",
    "        zeros(self.b_f)\n",
    "        zeros(self.b_c)\n",
    "        zeros(self.b_o)\n",
    "\n",
    "    def _set_hidden_state(self, X, H):\n",
    "        if H is None:\n",
    "            H = torch.zeros(X.shape[0], self.out_channels).to(X.device)\n",
    "        return H\n",
    "\n",
    "    def _set_cell_state(self, X, C):\n",
    "        if C is None:\n",
    "            C = torch.zeros(X.shape[0], self.out_channels).to(X.device)\n",
    "        return C\n",
    "\n",
    "    def _calculate_input_gate(self, X, edge_index, edge_weight, H, C, lambda_max):\n",
    "        I = torch.matmul(X, self.W_i)\n",
    "        I = I + self.conv_i(H, edge_index, edge_weight, lambda_max=lambda_max)\n",
    "        I = I + self.b_i\n",
    "        I = torch.sigmoid(I)\n",
    "        return I\n",
    "\n",
    "    def _calculate_forget_gate(self, X, edge_index, edge_weight, H, C, lambda_max):\n",
    "        F = torch.matmul(X, self.W_f)\n",
    "        F = F + self.conv_f(H, edge_index, edge_weight, lambda_max=lambda_max)\n",
    "        F = F + self.b_f\n",
    "        F = torch.sigmoid(F)\n",
    "        return F\n",
    "\n",
    "    def _calculate_cell_state(self, X, edge_index, edge_weight, H, C, I, F, lambda_max):\n",
    "        T = torch.matmul(X, self.W_c)\n",
    "        T = T + self.conv_c(H, edge_index, edge_weight, lambda_max=lambda_max)\n",
    "        T = T + self.b_c\n",
    "        T = torch.tanh(T)\n",
    "        C = F * C + I * T\n",
    "        return C\n",
    "\n",
    "    def _calculate_output_gate(self, X, edge_index, edge_weight, H, C, lambda_max):\n",
    "        O = torch.matmul(X, self.W_o)\n",
    "        O = O + self.conv_o(H, edge_index, edge_weight, lambda_max=lambda_max)\n",
    "        O = O + self.b_o\n",
    "        O = torch.sigmoid(O)\n",
    "        return O\n",
    "\n",
    "    def _calculate_hidden_state(self, O, C):\n",
    "        H = O * torch.tanh(C)\n",
    "        return H\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        X: torch.FloatTensor,\n",
    "        edge_index: torch.LongTensor,\n",
    "        edge_weight: torch.FloatTensor = None,\n",
    "        H: torch.FloatTensor = None,\n",
    "        C: torch.FloatTensor = None,\n",
    "        lambda_max: torch.Tensor = None,\n",
    "    ) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        Making a forward pass. If edge weights are not present the forward pass\n",
    "        defaults to an unweighted graph. If the hidden state and cell state\n",
    "        matrices are not present when the forward pass is called these are\n",
    "        initialized with zeros.\n",
    "\n",
    "        Arg types:\n",
    "            * **X** *(PyTorch Float Tensor)* - Node features.\n",
    "            * **edge_index** *(PyTorch Long Tensor)* - Graph edge indices.\n",
    "            * **edge_weight** *(PyTorch Long Tensor, optional)* - Edge weight vector.\n",
    "            * **H** *(PyTorch Float Tensor, optional)* - Hidden state matrix for all nodes.\n",
    "            * **C** *(PyTorch Float Tensor, optional)* - Cell state matrix for all nodes.\n",
    "            * **lambda_max** *(PyTorch Tensor, optional but mandatory if normalization is not sym)* - Largest eigenvalue of Laplacian.\n",
    "\n",
    "        Return types:\n",
    "            * **H** *(PyTorch Float Tensor)* - Hidden state matrix for all nodes.\n",
    "            * **C** *(PyTorch Float Tensor)* - Cell state matrix for all nodes.\n",
    "        \"\"\"\n",
    "        H = self._set_hidden_state(X, H)\n",
    "        C = self._set_cell_state(X, C)\n",
    "        I = self._calculate_input_gate(X, edge_index, edge_weight, H, C, lambda_max)\n",
    "        F = self._calculate_forget_gate(X, edge_index, edge_weight, H, C, lambda_max)\n",
    "        C = self._calculate_cell_state(X, edge_index, edge_weight, H, C, I, F, lambda_max)\n",
    "        O = self._calculate_output_gate(X, edge_index, edge_weight, H, C, lambda_max)\n",
    "        H = self._calculate_hidden_state(O, C)\n",
    "        return H, C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "def create_mock_data(number_of_nodes, edge_per_node, in_channels):\n",
    "    \"\"\"\n",
    "    Creating a mock feature matrix and edge index.\n",
    "    \"\"\"\n",
    "    graph = nx.watts_strogatz_graph(number_of_nodes, edge_per_node, 0.5)\n",
    "    edge_index = torch.LongTensor(np.array([edge for edge in graph.edges()]).T)\n",
    "    X = torch.FloatTensor(np.random.uniform(-1, 1, (number_of_nodes, in_channels)))\n",
    "    return X, edge_index\n",
    "\n",
    "def create_mock_edge_weight(edge_index):\n",
    "    \"\"\"\n",
    "    Creating a mock edge weight tensor.\n",
    "    \"\"\"\n",
    "    return torch.FloatTensor(np.random.uniform(0, 1, (edge_index.shape[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric_temporal.nn.recurrent import GCLSTM\n",
    "\n",
    "def test_gc_lstm_layer():\n",
    "    \"\"\"\n",
    "    Testing the GCLSTM Layer.\n",
    "    \"\"\"\n",
    "    number_of_nodes = 100\n",
    "    edge_per_node = 10\n",
    "    in_channels = 64\n",
    "    out_channels = 16\n",
    "    K = 2\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    X, edge_index = create_mock_data(number_of_nodes, edge_per_node, in_channels)\n",
    "    X = X.to(device)\n",
    "    edge_index = edge_index.to(device)\n",
    "    edge_weight = create_mock_edge_weight(edge_index).to(device)\n",
    "\n",
    "    layer = GCLSTM(in_channels=in_channels, out_channels=out_channels, K=K).to(device)\n",
    "\n",
    "    H, C = layer(X, edge_index, edge_weight)\n",
    "\n",
    "    assert H.shape == (number_of_nodes, out_channels)\n",
    "    assert C.shape == (number_of_nodes, out_channels)\n",
    "\n",
    "    H, C = layer(X, edge_index, edge_weight, H, C)\n",
    "\n",
    "    assert H.shape == (number_of_nodes, out_channels)\n",
    "    assert C.shape == (number_of_nodes, out_channels)\n",
    "\n",
    "    H, C = layer(X, edge_index, edge_weight, H, C)\n",
    "\n",
    "    assert H.shape == (number_of_nodes, out_channels)\n",
    "    assert C.shape == (number_of_nodes, out_channels)\n",
    "    \n",
    "    H, C = layer(X, edge_index, edge_weight, H, C)\n",
    "\n",
    "    assert H.shape == (number_of_nodes, out_channels)\n",
    "    assert C.shape == (number_of_nodes, out_channels)\n",
    "\n",
    "    H, C = layer(X, edge_index, edge_weight, H, C)\n",
    "\n",
    "    assert H.shape == (number_of_nodes, out_channels)\n",
    "    assert C.shape == (number_of_nodes, out_channels)\n",
    "\n",
    "    return H, C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[ 0.1355, -0.0395,  0.0161,  ...,  0.1816,  0.2447,  0.2828],\n",
      "        [ 0.1532,  0.0786, -0.2542,  ...,  0.5507, -0.1255,  0.7104],\n",
      "        [-0.2221,  0.3131, -0.1857,  ...,  0.4177, -0.0554, -0.2173],\n",
      "        ...,\n",
      "        [-0.2616, -0.3723,  0.1542,  ..., -0.0653,  0.1865, -0.1065],\n",
      "        [-0.0332,  0.2956,  0.3387,  ..., -0.1579, -0.1888,  0.1058],\n",
      "        [-0.1352, -0.3271,  0.3456,  ..., -0.0144,  0.2845,  0.0185]],\n",
      "       device='cuda:0', grad_fn=<MulBackward0>), tensor([[ 0.3737, -0.0648,  0.0260,  ...,  0.5058,  0.7231,  0.7706],\n",
      "        [ 0.9862,  0.1098, -0.3813,  ...,  0.9682, -0.2790,  1.9148],\n",
      "        [-0.5944,  0.9268, -0.3892,  ...,  1.0067, -0.1177, -0.5946],\n",
      "        ...,\n",
      "        [-0.5636, -0.8112,  0.4369,  ..., -0.1128,  0.4163, -0.5792],\n",
      "        [-0.1270,  0.4844,  1.0946,  ..., -0.3207, -0.4955,  0.2116],\n",
      "        [-0.4751, -0.5604,  0.5938,  ..., -0.0188,  0.8124,  0.0247]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>))\n"
     ]
    }
   ],
   "source": [
    "print(test_gc_lstm_layer())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "STGTrans",
   "language": "python",
   "name": "stgtrans"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
